{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_csv(\"split_data/val_features.csv\")\n",
    "y_val = pd.read_csv(\"split_data/val_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat([X_val, y_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val['row_prop_missing'] = df_val.isna().mean(axis=1) # Not ok to already calculate here - some are missing by design (e.g. sticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Rows with many NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_val[df_val['row_prop_missing'] > .10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Observations / remarks:\n",
    "- Missing values are partly houses and partly appartments -> can be assigned accordingly\n",
    "- There are synonyms -> can be grouped together\n",
    "- 'Andere' is never an appartment\n",
    "- Group infrequent levels together? Reduce dimensionality (but: not really necessary for DT-based approach?)\n",
    "- Perform clustering a.o. on price to reduce number of levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_val['subtype'], df_val['is_appartment'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/map_infrequent_subtypes.pkl', 'rb') as file:\n",
    "    map_infrequent_subtypes = pickle.load(file)\n",
    "\n",
    "df_val['subtype_regrouped'] = df_val['subtype'].apply(lambda x: 'Andere' if x in map_infrequent_subtypes else x) # remove this step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_synonyms = {\n",
    "    'Assistentie-appartement': 'Serviceflat',\n",
    "    'Villa-landhuis': 'Villa',\n",
    "    'Moderne villa': 'Villa',\n",
    "    'Eengezinswoning': 'Woning',    \n",
    "    # 'Herenwoning': 'Herenhuis',\n",
    "    # 'Dakappartement': 'Penthouse',\n",
    "    'Studio met slaaphoek': 'Studio',\n",
    "\n",
    "    # 'Rijwoning': 'Woning',\n",
    "    'Gelijkvloers app.': 'Appartement',\n",
    "    'Uitzonderlijke woning': 'Villa',\n",
    "    'Herenwoning': 'Villa',\n",
    "    'Herenhuis': 'Villa',\n",
    "    'Burgerswoning': 'Woning',\n",
    "    'Koppelwoning': 'Woning',\n",
    "    'Duplex': 'Appartement',\n",
    "    'Triplex': 'Appartement',\n",
    "    'Bungalow': 'Woning',\n",
    "    'Hoeve': 'Villa',\n",
    "    'Fermette': 'Woning',\n",
    "    'Bel-Ã©tage': 'Woning',\n",
    "    'Hoekwoning': 'Woning',\n",
    "    'Pastorijwoning': 'Woning',\n",
    "    'Arbeiderswoning': 'Woning',\n",
    "    'Loft': 'Loft Penthouse',\n",
    "    'Dakappartement': 'Appartement',\n",
    "    'Penthouse': 'Loft Penthouse',\n",
    "    'Chalet': 'Andere',\n",
    "    'Cottage': 'Andere',\n",
    "    'Vakantiewoning': 'Andere'\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "df_val['subtype_regrouped'] = df_val['subtype_regrouped'].replace(map_synonyms)\n",
    "\n",
    "df_val['subtype_regrouped'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Impute area with median value for subtype (regrouped) and province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['area_missing'] = df_val['area'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_area.pkl', 'rb') as file:\n",
    "    median_area = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['area_imputed'] = df_val.apply(\n",
    "    lambda row: median_area.get((row['subtype_regrouped'], row['province']), row['area']) if pd.isna(row['area']) else row['area'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['area_rel_to_bedrooms'] = df_val['area_imputed'] / (df_val['bedrooms'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Energy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only first letter of energy label, except for 'a+'\n",
    "\n",
    "df_val['energy_label_regrouped'] = df_val['energy_label'].apply(lambda x: x[0] if isinstance(x, str) and x != 'a+' and x != 'a+' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_val['energy_label_regrouped'], df_val['new_building'], dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "For f and g not ok: normally label A corresponds to values 0-100, B to 101-200, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['energy_value_missing'] = df_val['energy_value'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_energy.pkl', 'rb') as file:\n",
    "    median_energy = pickle.load(file)\n",
    "\n",
    "df_val['energy_value_imputed'] = df_val.apply(\n",
    "    lambda row: median_energy.get((row['energy_label_regrouped'], row['new_building'], row['subtype_regrouped']), row['energy_value']) if pd.isna(row['energy_value']) else row['energy_value'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all rows except for the ones where the energy label is also missing have an energy value\n",
    "\n",
    "df_val['energy_value_imputed'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.groupby(['subtype_regrouped', 'new_building'])['energy_value'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_energy_wo_label.pkl', 'rb') as file:\n",
    "    median_energy_wo_label = pickle.load(file)\n",
    "\n",
    "df_val['energy_value_imputed'] = df_val.apply(\n",
    "    lambda row: median_energy_wo_label.get((row['new_building'], row['subtype_regrouped']), row['energy_value']) if pd.isna(row['energy_value_imputed']) else row['energy_value_imputed'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All NAs imputed\n",
    "\n",
    "df_val['energy_value_imputed'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Advertiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "High number of categories - apply something similar to Weights Of Evidence (= for classification problems)\n",
    "\n",
    "Note: some advertisers occur only once and seem to have a person's name (not a real estate agency) -> also informative\n",
    "\n",
    "Make bins of advertisers based on how often they occur (only once (category 5: person) vs. more (categories 1-4: agency)) and their median price.\n",
    "Category 5: only occurs once, so no relevant information on 'typical' pricing; this advertiser will (normally) not occur in the test set either \n",
    "Catgories 1-4: occurs more than once, categorize based on median price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['advertiser'].value_counts(dropna = False).head(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_price_advertiser.pkl', 'rb') as file:\n",
    "    median_price_advertiser = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.merge(df_val, median_price_advertiser, how='left', on='advertiser')\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['advertiser_bin'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['advertiser_bin'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['advertiser_bin'] = np.where(pd.isna(df_val['advertiser_bin']), 5, df_val['advertiser_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['advertiser_bin'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_price_per_advertiser_bin.pkl', 'rb') as file:\n",
    "    median_price_per_advertiser_bin = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.merge(df_val, median_price_per_advertiser_bin, how='left', on='advertiser_bin')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Regional prices - Statbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/map_nis_deduplicated.pkl', 'rb') as file:\n",
    "    df_map_nis_deduplicated = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.merge(df_val, df_map_nis_deduplicated, how='left', left_on='postcode', right_on = 'zip_code')\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/statbel_prices_selection.pkl', 'rb') as file:\n",
    "    df_statbel_prices_selection = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.merge(df_val, df_statbel_prices_selection, how='left', left_on=['nis_code', 'is_appartment'], right_on = ['CD_REFNIS', 'F_APPARTMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still quite some missings, whereas profiling below suggests it's a highly relevant feature\n",
    "\n",
    "df_val['MS_P_50_median'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_lookup_zipcode.pkl', 'rb') as file:\n",
    "    price_lookup_zipcode = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"zip_code_first2\"] = df_val.zip_code.astype(str).str[:2]\n",
    "\n",
    "df_val[\"MS_P_50_median_imputed\"] = df_val.apply(\n",
    "    lambda row: price_lookup_zipcode.get(\n",
    "        (row[\"zip_code_first2\"], row[\"is_appartment\"]),\n",
    "        row[\"MS_P_50_median\"]\n",
    "    ) if pd.isna(row[\"MS_P_50_median\"]) else row[\"MS_P_50_median\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_lookup_province.pkl', 'rb') as file:\n",
    "    price_lookup_province = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"zip_code_first1\"] = df_val.zip_code.astype(str).str[:1]\n",
    "\n",
    "df_val[\"MS_P_50_median_imputed\"] = df_val.apply(\n",
    "    lambda row: price_lookup_province.get(\n",
    "        (row[\"zip_code_first1\"], row[\"is_appartment\"]),\n",
    "        row[\"MS_P_50_median\"]\n",
    "    ) if pd.isna(row[\"MS_P_50_median_imputed\"]) else row[\"MS_P_50_median_imputed\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[df_val['MS_P_50_median_imputed'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## Regional prices - Price per area "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Profile again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave out percentiles 25 and 75 for prices per NIS code because of multicollinearity\n",
    "# Leave out row count because it's not available when making a single prediction and has low predictive power (it's usually just 1)\n",
    "\n",
    "df_val_sel = df_val[['bedrooms', 'new_building', 'foto_amount', 'province', 'subtype_regrouped', 'area_missing', 'area_imputed', 'area_rel_to_bedrooms', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_TOTAL_TRANSACTIONS', 'MS_P_50_median_imputed', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Final variable selection and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_val_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val_sel = df_val_sel[df_val_sel['price'] < 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_preprocessed = df_val_sel.drop('price', axis = 1)\n",
    "X_val_preprocessed = pd.get_dummies(X_val_preprocessed, columns=['subtype_regrouped', 'province'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_preprocessed = df_val_sel['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_preprocessed, X_val_preprocessed, y_calib_preprocessed, y_val_preprocessed = train_test_split(X_val_preprocessed, y_val_preprocessed, test_size=.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_calib_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_preprocessed.to_csv(\"split_data/calib_features_preprocessed.csv\", index=False)\n",
    "y_calib_preprocessed.to_csv(\"split_data/calib_target_preprocessed.csv\", index=False)\n",
    "\n",
    "X_val_preprocessed.to_csv(\"split_data/val_features_preprocessed.csv\", index=False)\n",
    "y_val_preprocessed.to_csv(\"split_data/val_target_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
