{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "See https://www.kaggle.com/code/carlmcbrideellis/regression-prediction-intervals-with-mapie/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mapie.regression import MapieQuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"split_data/train_features_preprocessed.csv\")\n",
    "y_train = pd.read_csv(\"split_data/train_target_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib = pd.read_csv(\"split_data/calib_features_preprocessed.csv\")\n",
    "y_calib = pd.read_csv(\"split_data/calib_target_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_calib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_csv(\"split_data/val_features_preprocessed.csv\")\n",
    "y_val = pd.read_csv(\"split_data/val_target_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # for 90% target coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regressor = LGBMRegressor( n_estimators       = 1000,\n",
    "                           learning_rate      = 0.05, \n",
    "                           max_depth          = 7, \n",
    "                           min_child_samples  = 8,\n",
    "                           random_state       = 42,\n",
    "                           objective          = 'quantile',\n",
    "                           alpha              = alpha,\n",
    "                           verbose = 50\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = LGBMRegressor(objective='regression', metric='mae', verbose = 50)\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'num_leaves': [30, 50], # not ok: should be <= 2^max_depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [5, 7],\n",
    "    'min_child_samples': [5, 8],\n",
    "    'n_estimators' : [500, 1000]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Use negative MAE because sklearn wants higher values to be better\n",
    "    cv=5,                               # 5-fold cross-validation                 \n",
    "    n_jobs=-1                           # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.125 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LGBMRegressor( n_estimators       = 500,\n",
    "                           learning_rate      = 0.05, \n",
    "                           max_depth          = 7, \n",
    "                           min_child_samples  = 8,\n",
    "                           random_state       = 16,\n",
    "                           num_leaves         = 50,\n",
    "                           objective          = 'quantile',\n",
    "                           alpha              = alpha,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie = MapieQuantileRegressor(estimator=regressor, cv=\"split\", alpha=alpha)\n",
    "mapie.fit(X_train, np.ravel(y_train), X_calib=X_calib, y_calib=np.ravel(y_calib))\n",
    "y_pred, y_pis = mapie.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mapie = np.ravel(y_val)\n",
    "predictions_mapie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mapie.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mapie = y_val\n",
    "#predictions_mapie.columns = ['y_true']\n",
    "predictions_mapie[\"point prediction\"] = y_pred\n",
    "predictions_mapie[\"lower\"] = y_pis.reshape(-1,2)[:,0]\n",
    "predictions_mapie[\"upper\"] = y_pis.reshape(-1,2)[:,1]\n",
    "predictions_mapie[\"abs_error\"] = abs(predictions_mapie [\"point prediction\"] - predictions_mapie [\"y_true\"])\n",
    "# take a quick look\n",
    "predictions_mapie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def WIS_and_coverage(y_true, lower, upper, alpha):\n",
    "        \n",
    "        if np.isnan(lower)  == True: \n",
    "            raise ParticipantVisibleError(\"lower interval value contains NaN value(s)\")\n",
    "        if np.isinf(lower)  == True: \n",
    "            raise ParticipantVisibleError(\"lower interval value contains inf values(s)\")\n",
    "        if np.isnan(upper)  == True: \n",
    "            raise ParticipantVisibleError(\"upper interval value contains NaN value(s)\")\n",
    "        if np.isinf(upper)  == True: \n",
    "            raise ParticipantVisibleError(\"upper interval value contains inf values(s)\")\n",
    "        # These should not occur in a competition setting\n",
    "        if np.isnan(y_true) == True:\n",
    "            raise ParticipantVisibleError(\"y_true contains NaN value(s)\")\n",
    "        if np.isinf(y_true) == True: \n",
    "            raise ParticipantVisibleError(\"y_true contains inf values(s)\")\n",
    "        \n",
    "        # WIS for a single interval\n",
    "        score = np.abs(upper - lower)\n",
    "        if y_true < np.minimum(upper, lower):\n",
    "            score += ((2/alpha) * (np.minimum(upper, lower) - y_true))\n",
    "        if y_true > np.maximum(upper, lower):\n",
    "            score += ((2/alpha) * (y_true - np.maximum(upper, lower)))\n",
    "        # coverage for one single row\n",
    "        coverage  = 1\n",
    "        if (y_true < np.minimum(upper, lower)) or (y_true > np.maximum(upper, lower)):\n",
    "            coverage = 0\n",
    "        return score, coverage\n",
    "\n",
    "# vectorize the function\n",
    "v_WIS_and_coverage = np.vectorize(WIS_and_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(y_true, lower, upper, alpha):\n",
    "        \n",
    "        y_true = y_true.astype(float)\n",
    "        lower  = lower.astype(float)\n",
    "        upper  = upper.astype(float)\n",
    "        \n",
    "        WIS_score,coverage = v_WIS_and_coverage(y_true, lower, upper, alpha)\n",
    "        MWIS     = np.mean(WIS_score)\n",
    "        coverage = coverage.sum() / coverage.shape[0]\n",
    "        \n",
    "        MWIS      = float(MWIS)\n",
    "        coverage  = float(coverage)\n",
    "        \n",
    "        return MWIS, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions_mapie[\"y_true\"], predictions_mapie[\"lower\"], predictions_mapie[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mapie[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/mapie.pkl', 'wb') as file:\n",
    "    pickle.dump(mapie, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Gradient boosting regressor (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Alternative for MAPIE:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbmodel = GradientBoostingRegressor(loss = 'absolute_error')\n",
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=9,\n",
    "    min_samples_split=9,\n",
    ")\n",
    "for alpha in [0.10, 0.5, 0.90]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbmodel = GradientBoostingRegressor(loss = 'absolute_error')\n",
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=9,\n",
    "    min_samples_split=9,\n",
    ")\n",
    "for alpha in [0.10, 0.5, 0.90]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbmodel = GradientBoostingRegressor(loss = 'absolute_error')\n",
    "\n",
    "# {'learning_rate': 0.1,\n",
    "#  'max_depth': 10,\n",
    "#  'min_samples_leaf': 20,\n",
    "#  'min_samples_split': 60}\n",
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=20,\n",
    "    min_samples_split=60,\n",
    ")\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_calib)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_calib)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_calib)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_calib\n",
    "# predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error_med\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error_mid\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[5, 10, 15, 20],\n",
    "    min_samples_leaf=[1, 5, 10, 20, 25],\n",
    "    min_samples_split=[20, 30, 50, 60, 70],\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Use negative MAE because sklearn wants higher values to be better\n",
    "    cv=5,                               # 5-fold cross-validation                 \n",
    "    n_jobs=-1                           # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/gbr_sklearn_models.pkl', 'wb') as file:\n",
    "    pickle.dump(all_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_val)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_val)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_val)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_val\n",
    "#predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_val\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error_med\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## With lat/lon instead of province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = pd.read_csv(\"split_data/train_features_preprocessed_2.csv\")\n",
    "y_train_2 = pd.read_csv(\"split_data/train_target_preprocessed_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np.ravel(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[5, 10, 15, 20, 30, 40],\n",
    "    min_samples_leaf=[10, 20, 25, 30, 40],\n",
    "    min_samples_split=[20, 30, 50, 60, 70],\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Use negative MAE because sklearn wants higher values to be better\n",
    "    cv=5,                               # 5-fold cross-validation                 \n",
    "    n_jobs=-1                           # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    min_samples_leaf=40,\n",
    "    min_samples_split=20,\n",
    ")\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_2 = pd.read_csv(\"split_data/calib_features_preprocessed_2.csv\")\n",
    "y_calib_2 = pd.read_csv(\"split_data/calib_target_preprocessed_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_calib_2)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_calib_2)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_calib_2)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_calib_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_calib\n",
    "# predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_2\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/gbr_sklearn_models_2.pkl', 'wb') as file:\n",
    "    pickle.dump(all_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Attempt 3\n",
    "\n",
    "Not uploaded because worse results than attempt 2\n",
    "\n",
    "- Area is imputed taking number of bedrooms into account in addition to subtype\n",
    "- A few cases with latitudes and longitudes outside of Belgium are removed --> improved quality of imputed (mean) lat and lon\n",
    "- Statbel data no longer used (external source that needs to be updated): take median price per area (!) per type (is appartment Y/N) per zipcode from training data\n",
    "- <s>Flag price drop added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = pd.read_csv(\"split_data/train_features_preprocessed_3.csv\")\n",
    "y_train_3 = pd.read_csv(\"split_data/train_target_preprocessed_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = X_train_3.drop(['price_dropped', 'lon_missing'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3 = np.ravel(y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# Define hyperparameters grid\n",
    "# param_grid = dict(\n",
    "#     learning_rate=[0.05, 0.1, 0.2],\n",
    "#     max_depth=[5, 10, 15, 20, 25, 30, 35],\n",
    "#     min_samples_leaf=[20, 30, 40, 50],\n",
    "#     min_samples_split=[20, 30, 50, 60, 70],\n",
    "# )\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [15, 20, 25, 30, 35],\n",
    "    'min_samples_split': [10, 15, 20, 25],\n",
    "    'min_samples_leaf': [5, 10, 15, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 150]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Use negative MAE because sklearn wants higher values to be better\n",
    "    cv=5,                               # 5-fold cross-validation                 \n",
    "    n_jobs=-1                           # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_3, y_train_3)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=150,\n",
    "    max_depth=35,\n",
    "    min_samples_leaf=20,\n",
    "    min_samples_split=15,\n",
    ")\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_3 = pd.read_csv(\"split_data/calib_features_preprocessed_3.csv\")\n",
    "y_valid_3 = pd.read_csv(\"split_data/calib_target_preprocessed_3.csv\")\n",
    "\n",
    "X_valid_3 = X_valid_3.drop('price_dropped', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_valid_3)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_valid_3)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_valid_3)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_valid_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_valid_3\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Worst predictions on most expensive (outliers?) houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "predictions.sort_values(\"abs_error\", ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_3[y_valid_3['price'] == 999000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "## Attempt 3b\n",
    "\n",
    "Changes applied in attempt 3, but/and:\n",
    "- Use Statbel data again instead of median prices calculated based on training data\n",
    "- Median price per area: calculated per group of first 3 zipcode digits (more granular) instead of first 2\n",
    "- Flag indicating if last 2 (but not 3) or last 3 digits are 0 - indication of bigger cities?\n",
    "\n",
    "Last two changes are attempts to predict higher prices better (regional differences related to price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3b = pd.read_csv(\"split_data/train_features_preprocessed_3b.csv\")\n",
    "y_train_3b = pd.read_csv(\"split_data/train_target_preprocessed_3b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3b = np.ravel(y_train_3b)\n",
    "X_train_3b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# param_grid = {\n",
    "#     'max_depth': [25, 30, 35],\n",
    "#     'min_samples_split': [10, 15, 20, 25],\n",
    "#     'min_samples_leaf': [5, 10, 15, 20],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1]\n",
    "#     #, 'n_estimators': [100, 150]\n",
    "# }\n",
    "\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    n_estimators = [100, 150, 200],\n",
    "    max_depth=[20, 25, 30, 35],\n",
    "    min_samples_leaf=[20, 30, 40],\n",
    "    min_samples_split=[15, 20, 25, 30]    \n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  # Use negative MAE because sklearn wants higher values to be better\n",
    "    cv=5,                               # 5-fold cross-validation                 \n",
    "    n_jobs=-1                           # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_3b, y_train_3b)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_samples_leaf=30,\n",
    "    min_samples_split=20,\n",
    ")\n",
    "\n",
    "# try params of model 2\n",
    "# common_params = dict(\n",
    "#     learning_rate=0.1,\n",
    "#     n_estimators=200,\n",
    "#     max_depth=30,\n",
    "#     min_samples_leaf=40,\n",
    "#     min_samples_split=20,\n",
    "# )\n",
    "\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_3b, y_train_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_3b, y_train_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_3b = pd.read_csv(\"split_data/calib_features_preprocessed_3b.csv\")\n",
    "y_calib_3b = pd.read_csv(\"split_data/calib_target_preprocessed_3b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_calib_3b)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_calib_3b)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_calib_3b)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_calib_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_calib\n",
    "# predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_3b\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort_values(\"abs_error\", ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_3b[y_calib_3b['price'] == 835000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "## Test TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TABPFN_ALLOW_CPU_LARGE_DATASET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_1 = pd.read_csv(\"split_data/train_features_preprocessed_3_1.csv\")\n",
    "y_train_3_1 = pd.read_csv(\"split_data/train_target_preprocessed_3_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3_1 = np.ravel(y_train_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(TabPFNRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = TabPFNRegressor(device = \"cpu\", fit_mode = \"low_memory\")  \n",
    "regressor.fit(X_train_3_1, y_train_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_2 = pd.read_csv(\"split_data/calib_features_preprocessed_2.csv\")\n",
    "y_calib_2 = pd.read_csv(\"split_data/calib_target_preprocessed_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(X_calib_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_calib\n",
    "# predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_2\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "#predictions[\"med\"] = y_med\n",
    "#predictions[\"lower\"] = y_lower\n",
    "#predictions[\"upper\"] = y_upper\n",
    "#predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "#predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "#predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict quantiles\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "quantile_predictions = regressor.predict(\n",
    "    X_calib_2,\n",
    "    output_type=\"quantiles\",\n",
    "    quantiles=quantiles,\n",
    ")\n",
    "#for q, q_pred in zip(quantiles, quantile_predictions):\n",
    "#    print(f\"Quantile {q} MAE:\", mean_absolute_error(y_test, q_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_2\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = quantile_predictions[1]\n",
    "predictions[\"lower\"] = quantile_predictions[0]\n",
    "predictions[\"upper\"] = quantile_predictions[2]\n",
    "predictions[\"midpoint\"] = (quantile_predictions[0]+quantile_predictions[2])/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### With larger (almost 10000) training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_1 = pd.read_csv(\"split_data/train_features_preprocessed_4_1.csv\")\n",
    "y_train_4_1 = pd.read_csv(\"split_data/train_target_preprocessed_4_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_4_1 = np.ravel(y_train_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor4 = TabPFNRegressor(device = \"cpu\")  \n",
    "# regressor4.fit(X_train_4_1, y_train_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = regressor4.predict(X_calib_2)\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = y_calib\n",
    "# predictions.columns = ['y_true']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_2\n",
    "predictions[\"point prediction\"] = pred_y\n",
    "#predictions[\"med\"] = y_med\n",
    "#predictions[\"lower\"] = y_lower\n",
    "#predictions[\"upper\"] = y_upper\n",
    "#predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "#predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "#predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.05, 0.5, 0.95]\n",
    "quantile_predictions4 = regressor4.predict(\n",
    "    X_calib_2,\n",
    "    output_type=\"quantiles\",\n",
    "    quantiles=quantiles,\n",
    ")\n",
    "\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib_2\n",
    "predictions[\"point prediction\"] = pred_y\n",
    "predictions[\"med\"] = quantile_predictions4[1]\n",
    "predictions[\"lower\"] = quantile_predictions4[0]\n",
    "predictions[\"upper\"] = quantile_predictions4[2]\n",
    "predictions[\"midpoint\"] = (quantile_predictions4[0]+quantile_predictions4[2])/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "# take a quick look\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
