{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"split_data/train_features_preprocessed.csv\")\n",
    "y_train = pd.read_csv(\"split_data/train_target_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in X_train.columns if c.startswith(('subtype_regrouped_', 'province_')) or c in ('area_imputed', 'bedrooms', 'median_price_advertiser_bin', 'advertiser_count', 'energy_value_imputed', 'MS_P_50_median_imputed', 'new_building', 'foto_amount')]\n",
    "\n",
    "X_train = X_train[cols]\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up parameters for XGBoost ---\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'eval_metric': 'rmse',  # Root Mean Squared Error (RMSE) for evaluation\n",
    "    'max_depth': 10,  # Control complexity (higher can lead to overfitting)\n",
    "    'learning_rate': 0.01,  # Smaller values help prevent overfitting\n",
    "    'subsample': 0.8,  # Randomly sample 80% of the data for each tree to prevent overfitting\n",
    "    'colsample_bytree': 0.8,  # Randomly sample 80% of features for each tree\n",
    "    'lambda': 1,  # L2 regularization (prevents overfitting)\n",
    "    'alpha': 0,  # L1 regularization (optional, useful for sparse data)\n",
    "    'n_jobs': -1,  # Use all available cores\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', eval_metric='mae')\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    # 'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring= 'neg_mean_absolute_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    #'eval_metric': 'rmse',  # Root Mean Squared Error (RMSE) for evaluation\n",
    "    'eval_metric': 'mae',  # MAE\n",
    "    'max_depth': 7,  # Control complexity (higher can lead to overfitting)\n",
    "    'learning_rate': 0.1,  # Smaller values help prevent overfitting\n",
    "    'subsample': 0.7,  # Randomly sample 80% of the data for each tree to prevent overfitting\n",
    "    'colsample_bytree': 0.7,  # Randomly sample 80% of features for each tree\n",
    "    'min_child_weight': 1,\n",
    "    'lambda': 1,  # L2 regularization (prevents overfitting)\n",
    "    'alpha': 0,  # L1 regularization (optional, useful for sparse data)\n",
    "    'n_jobs': -1,  # Use all available cores\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use Cross-Validation for better generalization ---\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1000,  # Max number of rounds to train\n",
    "    early_stopping_rounds=50,  # Stop if no improvement in 50 rounds\n",
    "    nfold=5,  # 5-fold cross-validation\n",
    "    stratified=False,  # Not necessary for regression but defaults to False\n",
    "    verbose_eval=50  # Print out results every 50 iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get the best boosting round from cross-validation ---\n",
    "# best_num_boost_round = cv_results['test-rmse-mean'].idxmin()\n",
    "best_num_boost_round = cv_results['test-mae-mean'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the final model with best boosting rounds ---\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_num_boost_round\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: Plot Feature Importances ---\n",
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(model, importance_type='weight', max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance by 'gain' (more useful for regression)\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
