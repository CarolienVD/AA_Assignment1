{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"split_data/train_features_preprocessed.csv\")\n",
    "y_train = pd.read_csv(\"split_data/train_target_preprocessed.csv\")\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_csv(\"split_data/val_features_preprocessed.csv\")\n",
    "y_val = pd.read_csv(\"split_data/val_target_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Function to calculate Winkler Score for predicted intervals\n",
    "\n",
    "https://www.kaggle.com/datasets/carlmcbrideellis/winkler-interval-score-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WIS_and_coverage(y_true, lower, upper, alpha):\n",
    "        \n",
    "        if np.isnan(lower)  == True: \n",
    "            raise ParticipantVisibleError(\"lower interval value contains NaN value(s)\")\n",
    "        if np.isinf(lower)  == True: \n",
    "            raise ParticipantVisibleError(\"lower interval value contains inf values(s)\")\n",
    "        if np.isnan(upper)  == True: \n",
    "            raise ParticipantVisibleError(\"upper interval value contains NaN value(s)\")\n",
    "        if np.isinf(upper)  == True: \n",
    "            raise ParticipantVisibleError(\"upper interval value contains inf values(s)\")\n",
    "        # These should not occur in a competition setting\n",
    "        if np.isnan(y_true) == True:\n",
    "            raise ParticipantVisibleError(\"y_true contains NaN value(s)\")\n",
    "        if np.isinf(y_true) == True: \n",
    "            raise ParticipantVisibleError(\"y_true contains inf values(s)\")\n",
    "        \n",
    "        # WIS for a single interval\n",
    "        score = np.abs(upper - lower)\n",
    "        if y_true < np.minimum(upper, lower):\n",
    "            score += ((2/alpha) * (np.minimum(upper, lower) - y_true))\n",
    "        if y_true > np.maximum(upper, lower):\n",
    "            score += ((2/alpha) * (y_true - np.maximum(upper, lower)))\n",
    "        # coverage for one single row\n",
    "        coverage  = 1\n",
    "        if (y_true < np.minimum(upper, lower)) or (y_true > np.maximum(upper, lower)):\n",
    "            coverage = 0\n",
    "        return score, coverage\n",
    "\n",
    "# vectorize the function\n",
    "v_WIS_and_coverage = np.vectorize(WIS_and_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, lower, upper, alpha):\n",
    "        \n",
    "        y_true = y_true.astype(float)\n",
    "        lower  = lower.astype(float)\n",
    "        upper  = upper.astype(float)\n",
    "        \n",
    "        WIS_score,coverage = v_WIS_and_coverage(y_true, lower, upper, alpha)\n",
    "        MWIS     = np.mean(WIS_score)\n",
    "        coverage = coverage.sum() / coverage.shape[0]\n",
    "        \n",
    "        MWIS      = float(MWIS)\n",
    "        coverage  = float(coverage)\n",
    "        \n",
    "        return MWIS, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Gradient boosting regressor\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Submission 1 to leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# hyperparameters grid\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[5, 10, 15, 20],\n",
    "    min_samples_leaf=[1, 5, 10, 20, 25],\n",
    "    min_samples_split=[20, 30, 50, 60, 70],\n",
    ")\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  \n",
    "    cv=5,                                      \n",
    "    n_jobs=-1                           \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "\n",
    "common_params = dict(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=20,\n",
    "    min_samples_split=60,\n",
    ")\n",
    "\n",
    "# for point prediction\n",
    "\n",
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train, y_train)\n",
    "\n",
    "# for interval prediction\n",
    "\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_val)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_val)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_val)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_calib\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error_med\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cases where point prediction does not fall in the predicted interval\n",
    "\n",
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for invalid cases, replace point prediction by midpoint of predicted interval\n",
    "\n",
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/gbr_sklearn_models.pkl', 'wb') as file:\n",
    "    pickle.dump(all_models, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Submission 2 to leaderboard\n",
    "\n",
    "With lat/lon instead of province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = pd.read_csv(\"split_data/train_features_preprocessed_2.csv\")\n",
    "y_train_2 = pd.read_csv(\"split_data/train_target_preprocessed_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_2 = pd.read_csv(\"split_data/val_features_preprocessed_2.csv\")\n",
    "y_val_2 = pd.read_csv(\"split_data/val_target_preprocessed_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np.ravel(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "# hyperparameters grid\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[5, 10, 15, 20, 30, 40],\n",
    "    min_samples_leaf=[10, 20, 25, 30, 40],\n",
    "    min_samples_split=[20, 30, 50, 60, 70],\n",
    ")\n",
    "\n",
    "# perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error', \n",
    "    cv=5,                                             \n",
    "    n_jobs=-1                         \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "\n",
    "common_params = dict(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    min_samples_leaf=40,\n",
    "    min_samples_split=20,\n",
    ")\n",
    "\n",
    "# for point prediction\n",
    "\n",
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_2, y_train_2)\n",
    "\n",
    "# for interval prediction\n",
    "\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_val_2)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_val_2)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_val_2)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_val_2\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/gbr_sklearn_models_2.pkl', 'wb') as file:\n",
    "    pickle.dump(all_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = all_models[\"q 0.50\"].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train_2.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train_2.shape[1]), X_train_2.columns[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Feature importance submission 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = all_models[\"mae\"].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train_2.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train_2.shape[1]), X_train_2.columns[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Attempt 3\n",
    "\n",
    "Not uploaded because worse results than attempt 2\n",
    "\n",
    "- Area is imputed taking number of bedrooms into account in addition to subtype\n",
    "- A few cases with latitudes and longitudes outside of Belgium are removed --> improved quality of imputed (mean) lat and lon\n",
    "- Statbel data no longer used (external source that needs to be updated): take median price per m2 per type (is appartment Y/N) per zipcode from training data\n",
    "- Flag price drop added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = pd.read_csv(\"split_data/train_features_preprocessed_3.csv\")\n",
    "y_train_3 = pd.read_csv(\"split_data/train_target_preprocessed_3.csv\")\n",
    "\n",
    "y_train_3 = np.ravel(y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [15, 20, 25, 30, 35],\n",
    "    'min_samples_split': [10, 15, 20, 25],\n",
    "    'min_samples_leaf': [5, 10, 15, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 150]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error', \n",
    "    cv=5,                                     \n",
    "    n_jobs=-1                           \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=150,\n",
    "    max_depth=35,\n",
    "    min_samples_leaf=20,\n",
    "    min_samples_split=15,\n",
    ")\n",
    "\n",
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_3, y_train_3)\n",
    "\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_3 = pd.read_csv(\"split_data/val_features_preprocessed_3.csv\")\n",
    "y_valid_3 = pd.read_csv(\"split_data/val_target_preprocessed_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_valid_3)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_valid_3)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_valid_3)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_valid_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_valid_3\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Check worst predictions attempt 3\n",
    "\n",
    "Worst predictions on most expensive houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort_values(\"abs_error\", ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Attempt 3b\n",
    "\n",
    "Changes applied in attempt 3, but\n",
    "- Use Statbel data again instead of median prices calculated based on training data\n",
    "- Median price per area: calculated per group of first 3 zipcode digits (more granular) instead of first 2\n",
    "- Flag indicating if last 2 (but not 3) or last 3 digits are 0 - indication of bigger cities?\n",
    "\n",
    "Last two changes are attempts to predict higher prices better (regional differences related to price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3b = pd.read_csv(\"split_data/train_features_preprocessed_3b.csv\")\n",
    "y_train_3b = pd.read_csv(\"split_data/train_target_preprocessed_3b.csv\")\n",
    "\n",
    "y_train_3b = np.ravel(y_train_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(loss=\"absolute_error\", verbose = 50)\n",
    "\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    n_estimators = [100, 150, 200],\n",
    "    max_depth=[20, 25, 30, 35],\n",
    "    min_samples_leaf=[20, 30, 40],\n",
    "    min_samples_split=[15, 20, 25, 30]    \n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',  \n",
    "    cv=5,                                                \n",
    "    n_jobs=-1                           \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_3b, y_train_3b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_samples_leaf=30,\n",
    "    min_samples_split=20,\n",
    ")\n",
    "\n",
    "gbr_mae = GradientBoostingRegressor(loss=\"absolute_error\", **common_params)\n",
    "all_models[\"mae\"] = gbr_mae.fit(X_train_3b, y_train_3b)\n",
    "\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train_3b, y_train_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_3b = pd.read_csv(\"split_data/calib_features_preprocessed_3b.csv\")\n",
    "y_val_3b = pd.read_csv(\"split_data/calib_target_preprocessed_3b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = all_models[\"mae\"].predict(X_val_3b)\n",
    "y_lower = all_models[\"q 0.05\"].predict(X_val_3b)\n",
    "y_upper = all_models[\"q 0.95\"].predict(X_val_3b)\n",
    "y_med = all_models[\"q 0.50\"].predict(X_val_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_val_3b\n",
    "predictions[\"point prediction\"] = y_pred\n",
    "predictions[\"med\"] = y_med\n",
    "predictions[\"lower\"] = y_lower\n",
    "predictions[\"upper\"] = y_upper\n",
    "predictions[\"midpoint\"] = (y_upper+y_lower)/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['point prediction'] = np.where(\n",
    "    (predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']),\n",
    "    predictions['point prediction'],\n",
    "    predictions['midpoint']\n",
    ")\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "# TabPFN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tabpfn import TabPFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"train_features_preprocessed_3b.csv\")\n",
    "y_train = pd.read_csv(\"train_target_preprocessed_3b.csv\")\n",
    "\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = TabPFNRegressor(n_estimators = 64, ignore_pretraining_limits=True, softmax_temperature = 0.6, average_before_softmax = True, categorical_features_indices = [0, 6, 8, 12, 13, 14])  \n",
    "reg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_3b = pd.read_csv(\"split_data/calib_features_preprocessed_3b.csv\")\n",
    "y_val_3b = pd.read_csv(\"split_data/calib_target_preprocessed_3b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = reg1.predict(X_val_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = reg1.predict(X_val_3b, output_type = \"quantiles\", quantiles = [0.05, 0.5, 0.95])\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['y_true'] = y_val_3b\n",
    "predictions[\"point prediction\"] = predictions1\n",
    "predictions[\"med\"] = predictions2[1]\n",
    "predictions[\"lower\"] = predictions2[0]\n",
    "predictions[\"upper\"] = predictions2[2]\n",
    "predictions[\"midpoint\"] = (predictions2[0]+predictions2[1])/2\n",
    "\n",
    "predictions[\"abs_error\"] = abs(predictions[\"point prediction\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_med\"] = abs(predictions[\"med\"] - predictions[\"y_true\"])\n",
    "predictions[\"abs_error_mid\"] = abs(predictions[\"midpoint\"] - predictions[\"y_true\"])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = predictions[~((predictions['point prediction'] >= predictions['lower']) & (predictions['point prediction'] <= predictions['upper']))]\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWIS, coverage = score(predictions[\"y_true\"], predictions[\"lower\"], predictions[\"upper\"], alpha = .20)\n",
    "\n",
    "MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"abs_error\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
