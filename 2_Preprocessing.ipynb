{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh split_data/train_target.csv\n",
    "!head split_data/train_target.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"split_data/train_features.csv\")\n",
    "# y_train = pd.read_csv(\"split_data/train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"split_data/train_target.csv\", names=[\"price\"], header = 0)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['row_prop_missing'] = df_train.isna().mean(axis=1) # Not ok to already calculate here - some are missing by design (e.g. sticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['added_time_formatted'] = pd.to_datetime(df_train['added_time'],unit='s')\n",
    "# display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Observations  / remarks:\n",
    "- id: make sure not to include\n",
    "- is_appartment: multicollinearity with subtype -> don't include both\n",
    "- area: high correlation with price! impute missing values based on median per subtype, add column to indicate original was missing\n",
    "- added_time: number of minutes / seconds since the property was added? Can be relevant (very expensive properties might take longer before being sold)\n",
    "- bedrooms: high correlation with area -> price!\n",
    "- new_building: no remarks\n",
    "- postcode: no remarks\n",
    "- lat / lon: impute missings? calculate distance to railway track, airport, highway (noise)?\n",
    "- advertiser: impute missings or treat as separate category? > 2000 unique values: reduce? \n",
    "- foto_amount: no remarks\n",
    "- is_promoted: constant -> ignore\n",
    "- subtype: limited number of missings, can be partly imputed based on is_appartment\n",
    "- sticker (new / price drop): only price drop could be relevant for price? recode missing values to third category 'no sticker'? ! perfect correlation with energy_value?\n",
    "- price drop date: relevance?\n",
    "- energy_value: many missings! can be partly imputed using new_building and/or energy_label? note: a missing energy value usually means it's a bad one\n",
    "- energy_label: many missings! multicollinearity with energy_value - value is more precise \n",
    "- province: maybe cross-check with postal code (DQ); use statbel average price per house / appartment data?\n",
    "- price: maximum 999999?\n",
    "- Outliers: Treat or not? DT-based methods can handle them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Rows with many NAs\n",
    "\n",
    "No removal done so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['sticker'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['row_prop_missing'] = df_train.isna().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train[df_train['row_prop_missing'] > .25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check done: row with most recent (max) added_time does not necessarily correspond to row with lowest price\n",
    "# For consistency, deduplicate by keeping row with lowest price (assumed to actually also be most recently updated) - this of course won't work in the test set\n",
    "# Use added_time\n",
    "\n",
    "# df_deduplicate = df_train.groupby([\"area\", \"bedrooms\", \"postcode\", \"lat\", \"lon\", \"advertiser\", \"subtype\"], dropna = False).agg({'price': ['nunique','min'], 'added_time': 'max'}).reset_index()\n",
    "# df_deduplicate.columns = list(map(''.join, df_deduplicate.columns.values))\n",
    "# df_deduplicate = df_deduplicate.rename(columns = {'pricemin' : 'price'})\n",
    "# df_deduplicate.head()\n",
    "\n",
    "df_deduplicate = df_train.groupby([\"area\", \"bedrooms\", \"postcode\", \"lat\", \"lon\", \"advertiser\", \"subtype\"], dropna = False).agg({'added_time': 'max', 'id': 'nunique'}).reset_index()\n",
    "df_deduplicate = df_deduplicate.rename(columns = {\"id\" : \"row_count\"})\n",
    "df_deduplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_deduplicate, how = 'inner', on = [\"area\", \"bedrooms\", \"postcode\", \"lat\", \"lon\", \"advertiser\", \"subtype\", \"added_time\"])\n",
    "df_train = df_train.sort_values(by='added_time', ascending=False)\n",
    "df_train['row_num'] = df_train.groupby([\"area\", \"bedrooms\", \"postcode\", \"lat\", \"lon\", \"advertiser\", \"subtype\"], dropna = False).cumcount() + 1\n",
    "df_train = df_train[df_train['row_num'] == 1].drop(columns='row_num')\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Observations / remarks:\n",
    "- Missing values are partly houses and partly appartments -> can be assigned accordingly\n",
    "- There are synonyms -> can be grouped together\n",
    "- 'Andere' is never an appartment\n",
    "- Group infrequent levels together? Reduce dimensionality (but: not really necessary for DT-based approach?)\n",
    "- Perform clustering a.o. on price to reduce number of levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['subtype'], df_train['is_appartment'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_counts = df_train['subtype'].value_counts()\n",
    "min_subtype_count = 10\n",
    "map_infrequent_subtypes = subtype_counts[subtype_counts < min_subtype_count].index\n",
    "\n",
    "df_train['subtype_regrouped'] = df_train['subtype'].apply(lambda x: 'Andere' if x in map_infrequent_subtypes else x) # remove this step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/map_infrequent_subtypes.pkl', 'wb') as file:\n",
    "    pickle.dump(map_infrequent_subtypes, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_synonyms = {\n",
    "    'Assistentie-appartement': 'Serviceflat',\n",
    "    'Villa-landhuis': 'Villa',\n",
    "    'Moderne villa': 'Villa',\n",
    "    'Eengezinswoning': 'Woning',    \n",
    "    # 'Herenwoning': 'Herenhuis',\n",
    "    # 'Dakappartement': 'Penthouse',\n",
    "    'Studio met slaaphoek': 'Studio',\n",
    "\n",
    "    # 'Rijwoning': 'Woning',\n",
    "    'Gelijkvloers app.': 'Appartement',\n",
    "    'Uitzonderlijke woning': 'Villa',\n",
    "    'Herenwoning': 'Villa',\n",
    "    'Herenhuis': 'Villa',\n",
    "    'Burgerswoning': 'Woning',\n",
    "    'Koppelwoning': 'Woning',\n",
    "    'Duplex': 'Appartement',\n",
    "    'Triplex': 'Appartement',\n",
    "    'Bungalow': 'Woning',\n",
    "    'Hoeve': 'Villa',\n",
    "    'Fermette': 'Woning',\n",
    "    'Bel-Ã©tage': 'Woning',\n",
    "    'Hoekwoning': 'Woning',\n",
    "    'Pastorijwoning': 'Woning',\n",
    "    'Arbeiderswoning': 'Woning',\n",
    "    'Loft': 'Loft Penthouse',\n",
    "    'Dakappartement': 'Appartement',\n",
    "    'Penthouse': 'Loft Penthouse',\n",
    "    'Chalet': 'Andere',\n",
    "    'Cottage': 'Andere',\n",
    "    'Vakantiewoning': 'Andere',\n",
    "    'Gemengd gebruik': 'Andere'\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "df_train['subtype_regrouped'] = df_train['subtype_regrouped'].replace(map_synonyms)\n",
    "df_train['subtype_regrouped'] = df_train['subtype_regrouped'].fillna('Andere')\n",
    "\n",
    "df_train['subtype_regrouped'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_median_price = df_train.groupby('subtype_regrouped')['price'].median().reset_index()\n",
    "subtype_median_price.columns = ['subtype_regrouped', 'subtype_median_price']\n",
    "subtype_median_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_median_price.to_pickle('intermediate_data/subtype_median_price.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, subtype_median_price, how = 'left', on = 'subtype_regrouped')\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Impute area with median value for subtype (regrouped) and province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['area_missing'] = df_train['area'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['area_missing'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('is_appartment')['area'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('is_appartment')['area'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('is_appartment')['area'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('is_appartment')['area'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_area = df_train.groupby(['subtype_regrouped', 'province'])['area'].median()\n",
    "display(median_area)\n",
    "median_area.to_pickle('intermediate_data/median_area.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group only based on subtype because some combinations with province results in missings and differences between provinces are small\n",
    "\n",
    "median_area = df_train.groupby('subtype_regrouped')['area'].median()\n",
    "display(median_area)\n",
    "median_area.to_pickle('intermediate_data/median_area.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 3: also group based on number of bedrooms\n",
    "\n",
    "median_area_bedrooms = df_train.groupby(['subtype_regrouped', 'bedrooms'])['area'].median()\n",
    "display(median_area_bedrooms)\n",
    "median_area_bedrooms.to_pickle('intermediate_data/median_area_bedrooms.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['area_imputed1'] = df_train.apply(\n",
    "    lambda row: median_area_bedrooms.get((row['subtype_regrouped'], row['bedrooms']), row['area']) if pd.isna(row['area']) else row['area'],\n",
    "    axis=1\n",
    ")\n",
    "df_train['area_imputed2'] = df_train.apply(\n",
    "    lambda row: median_area.get((row['subtype_regrouped']), row['area']) if pd.isna(row['area']) else row['area'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_train['area_imputed'] = df_train['area_imputed1'].combine_first(df_train['area_imputed2'])\n",
    "\n",
    "df_train.drop(['area_imputed1', 'area_imputed2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train[df_train['area_imputed'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['area_rel_to_bedrooms'] = df_train['area_imputed'] / (df_train['bedrooms'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Energy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['energy_label'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "New buildings with energy label d, e, f, g -> probably bad DQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['energy_label'], df_train['new_building'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only first letter of energy label, except for 'a+'\n",
    "\n",
    "df_train['energy_label_regrouped'] = df_train['energy_label'].apply(lambda x: x[0] if isinstance(x, str) and x != 'a+' and x != 'a+' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['energy_label_regrouped'], df_train['new_building'], dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "For f and g not ok: normally label A corresponds to values 0-100, B to 101-200, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['energy_label_regrouped'])['energy_value'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['energy_label_regrouped', 'new_building'])['energy_value'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['energy_value'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train[df_train['energy_value'].isna()]['energy_label_regrouped'], df_train[df_train['energy_value'].isna()]['new_building'], dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "For categories with missings, the sample size with valid energy values is large enough (see higher), so it's okay to also impute based on new_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_energy = df_train.groupby(['energy_label_regrouped', 'new_building', 'subtype_regrouped'])['energy_value'].median()\n",
    "\n",
    "median_energy.to_pickle('intermediate_data/median_energy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['energy_value_missing'] = df_train['energy_value'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['energy_value_imputed'] = df_train.apply(\n",
    "    lambda row: median_energy.get((row['energy_label_regrouped'], row['new_building'], row['subtype_regrouped']), row['energy_value']) if pd.isna(row['energy_value']) else row['energy_value'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['energy_value'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all rows except for the ones where the energy label is also missing have an energy value\n",
    "\n",
    "df_train['energy_value_imputed'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['subtype_regrouped', 'new_building'])['energy_value'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_energy_wo_label = df_train.groupby(['new_building', 'subtype_regrouped'])['energy_value'].median()\n",
    "\n",
    "median_energy_wo_label.to_pickle('intermediate_data/median_energy_wo_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['energy_value_imputed'] = df_train.apply(\n",
    "    lambda row: median_energy_wo_label.get((row['new_building'], row['subtype_regrouped']), row['energy_value']) if pd.isna(row['energy_value_imputed']) else row['energy_value_imputed'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All NAs imputed\n",
    "\n",
    "df_train['energy_value_imputed'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Advertiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "High number of categories - apply something similar to Weights Of Evidence (= for classification problems)\n",
    "\n",
    "Note: some advertisers occur only once and seem to have a person's name (not a real estate agency) -> also informative\n",
    "\n",
    "Make bins of advertisers based on how often they occur (only once (category 5: person) vs. more (categories 1-4: agency)) and their median price.\n",
    "Category 5: only occurs once, so no relevant information on 'typical' pricing; this advertiser will (normally) not occur in the train set either \n",
    "Catgories 1-4: occurs more than once, categorize based on median price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['advertiser'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['advertiser'].value_counts(dropna = False).head(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advertiser_counts = df_train['advertiser'].value_counts()\n",
    "# min_advertiser_count = 10\n",
    "# map_infrequent_advertisers = advertiser_counts[advertiser_counts < min_advertiser_count].index\n",
    "# df_train['advertiser_regrouped'] = df_train['advertiser'].apply(lambda x: 'Andere' if x in map_infrequent_advertisers or pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_advertiser = df_train.groupby('advertiser', dropna = False).agg(\n",
    "    advertiser_median_price=('price', 'median'),  \n",
    "    advertiser_count=('advertiser', 'size')     \n",
    ").reset_index()\n",
    "\n",
    "median_price_advertiser.columns = ['advertiser', 'advertiser_median_price', 'advertiser_count']\n",
    "display(median_price_advertiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_advertiser['advertiser_bin'] = np.where((median_price_advertiser['advertiser_count'] == 1) | (pd.isna(median_price_advertiser['advertiser']))\n",
    "                                                     , 5, np.nan)\n",
    "display(median_price_advertiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 4  \n",
    "median_price_advertiser.loc[(median_price_advertiser['advertiser_count'] > 1) & (~pd.isna(median_price_advertiser['advertiser'])), 'advertiser_bin'] = pd.qcut(\n",
    "    median_price_advertiser.loc[(median_price_advertiser['advertiser_count'] > 1) & ~ (pd.isna(median_price_advertiser['advertiser'])), 'advertiser_median_price'], \n",
    "    q=num_bins, \n",
    "    labels=False\n",
    ")\n",
    "display(median_price_advertiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_advertiser.groupby(['advertiser_bin'])['advertiser_median_price'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/median_price_advertiser.pkl', 'wb') as file:\n",
    "    pickle.dump(median_price_advertiser, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, median_price_advertiser, how='left', on = \"advertiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['advertiser_bin'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_per_advertiser_bin = df_train.groupby(['advertiser_bin'], as_index = False)['price'].median()\n",
    "median_price_per_advertiser_bin.columns = ['advertiser_bin', 'median_price_advertiser_bin']\n",
    "median_price_per_advertiser_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_per_advertiser_bin.to_pickle('intermediate_data/median_price_per_advertiser_bin.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, median_price_per_advertiser_bin, how='left', on='advertiser_bin')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## Regional prices - Statbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis = pd.read_csv(\"external_data/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_map_nis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis['zipCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are multiple lines for the same zip code, there is 1 'main'\n",
    "df_map_nis[df_map_nis['zipCode'] == 3700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis[df_map_nis['zipCode'] == 2260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis[df_map_nis['zipCode'] == 7700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis[df_map_nis['zipCode'] == 2260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis_renamed = df_map_nis.rename(columns={'name': 'municipality', 'province': 'province_nis', 'zipCode': 'zip_code', 'nisCode': 'nis_code'}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis_deduplicated = df_map_nis_renamed.sort_values(by='main', ascending=False).groupby('zip_code').first().reset_index().drop(columns = 'main').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis_deduplicated[df_map_nis_deduplicated['zip_code'] == 3700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_nis_deduplicated['zip_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/map_nis_deduplicated.pkl', 'wb') as file:\n",
    "    pickle.dump(df_map_nis_deduplicated, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_map_nis_deduplicated, how='left', left_on='postcode', right_on = 'zip_code')\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['province'], df_train['province_nis'], dropna=False) # Looks okay except for Antwerpen-Oost-Vlaanderen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices = pd.read_excel(\"external_data/vastgoed_2010_9999.xlsx\", sheet_name = '2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_statbel_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices['CD_TYPE_NL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices['CD_CLASS_SURFACE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_houses = df_statbel_prices[(df_statbel_prices['CD_TYPE_NL'] == 'Alle huizen met 2, 3, 4 of meer gevels (excl. appartementen)') & (df_statbel_prices['CD_PERIOD'] == 'S1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_houses = (\n",
    "    df_statbel_prices\n",
    "    .query(\"CD_TYPE_NL == 'Alle huizen met 2, 3, 4 of meer gevels (excl. appartementen)' & CD_PERIOD == 'S1'\")  \n",
    "    .loc[:, ['CD_REFNIS', 'CD_REFNIS_NL', 'MS_TOTAL_TRANSACTIONS', 'MS_P_25', 'MS_P_50_median', 'MS_P_75']]  \n",
    "    .assign(F_APPARTMENT = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_appartments = (\n",
    "    df_statbel_prices\n",
    "    .query(\"CD_TYPE_NL == 'Appartementen' & CD_PERIOD == 'S1'\")  \n",
    "    .loc[:, ['CD_REFNIS', 'CD_REFNIS_NL', 'MS_TOTAL_TRANSACTIONS', 'MS_P_25', 'MS_P_50_median', 'MS_P_75']]  \n",
    "    .assign(F_APPARTMENT = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection = pd.concat([df_statbel_prices_houses, df_statbel_prices_appartments], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_statbel_prices_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection[df_statbel_prices_selection['CD_REFNIS_NL'].str.contains('PROVINCIE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/statbel_prices_selection.pkl', 'wb') as file:\n",
    "    pickle.dump(df_statbel_prices_selection, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of NIS where there are no appartment prices available\n",
    "df_statbel_prices[df_statbel_prices['CD_REFNIS'] == 13049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_statbel_prices_selection, how='left', left_on=['nis_code', 'is_appartment'], right_on = ['CD_REFNIS', 'F_APPARTMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still quite some missings, whereas profiling below suggests it's a highly relevant feature\n",
    "\n",
    "df_train['MS_P_50_median'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median of MS_P_50_median per zip code and house vs. appartement for imputation\n",
    "\n",
    "df_statbel_prices_selection = pd.merge(df_statbel_prices_selection, df_map_nis_deduplicated, how='left', left_on=['CD_REFNIS'], right_on = ['nis_code'])\n",
    "display(df_statbel_prices_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection[\"zip_code_first2\"] = df_statbel_prices_selection.zip_code.astype(str).str[:2]\n",
    "median_price_per_zipcode = df_statbel_prices_selection.groupby(['zip_code_first2', 'F_APPARTMENT'], as_index = False)['MS_P_50_median'].median()\n",
    "median_price_per_zipcode.columns = ['zip_code_first2', 'is_appartment', 'med_MS_P_50_median']\n",
    "display(median_price_per_zipcode.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_per_zipcode['med_MS_P_50_median'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection[df_statbel_prices_selection['CD_REFNIS'] == 13049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices[df_statbel_prices['CD_REFNIS'] == 54007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_lookup_zipcode = median_price_per_zipcode.set_index(\n",
    "    ['zip_code_first2', 'is_appartment']\n",
    ")['med_MS_P_50_median'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_lookup_zipcode.pkl', 'wb') as file:\n",
    "    pickle.dump(price_lookup_zipcode, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"zip_code_first2\"] = df_train.postcode.astype(str).str[:2]\n",
    "\n",
    "df_train[\"MS_P_50_median_imputed\"] = df_train.apply(\n",
    "    lambda row: price_lookup_zipcode.get(\n",
    "        (row[\"zip_code_first2\"], row[\"is_appartment\"]),\n",
    "        row[\"MS_P_50_median\"]\n",
    "    ) if pd.isna(row[\"MS_P_50_median\"]) else row[\"MS_P_50_median\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection[df_statbel_prices_selection['CD_REFNIS'] == 13049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still quite some missings, whereas profiling below suggests it's a highly relevant feature\n",
    "\n",
    "df_train['MS_P_50_median_imputed'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['MS_P_50_median_imputed'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statbel_prices_selection[\"zip_code_first1\"] = df_statbel_prices_selection.zip_code.astype(str).str[:1]\n",
    "median_price_per_province = df_statbel_prices_selection.groupby(['zip_code_first1', 'F_APPARTMENT'], as_index = False)['MS_P_50_median'].median()\n",
    "median_price_per_province.columns = ['zip_code_first1', 'is_appartment', 'med_MS_P_50_median']\n",
    "display(median_price_per_province.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_lookup_province = median_price_per_province.set_index(\n",
    "    ['zip_code_first1', 'is_appartment']\n",
    ")['med_MS_P_50_median'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_lookup_province.pkl', 'wb') as file:\n",
    "    pickle.dump(price_lookup_province, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"zip_code_first1\"] = df_train.postcode.astype(str).str[:1]\n",
    "\n",
    "df_train[\"MS_P_50_median_imputed\"] = df_train.apply(\n",
    "    lambda row: price_lookup_province.get(\n",
    "        (row[\"zip_code_first1\"], row[\"is_appartment\"]),\n",
    "        row[\"MS_P_50_median\"]\n",
    "    ) if pd.isna(row[\"MS_P_50_median_imputed\"]) else row[\"MS_P_50_median_imputed\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['MS_P_50_median_imputed'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "## Regional prices - Price per area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['price_per_area'] = df_train['price'] / df_train['area']\n",
    "price_per_area_per_region = df_train.groupby('zip_code_first2')['price_per_area'].median().round(2).reset_index()\n",
    "price_per_area_per_region.columns = ['zip_code_first2', 'median_price_per_area']\n",
    "display(price_per_area_per_region.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_per_area_per_region.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_per_region, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, price_per_area_per_region, how = 'left', on = 'zip_code_first2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"zip_code_first3\"] = df_train.postcode.astype(str).str[:3]\n",
    "\n",
    "df_train['price_per_area_2'] = df_train['price'] / df_train['area']\n",
    "price_per_area_per_region_2 = df_train.groupby('zip_code_first3')['price_per_area'].median().round(2).reset_index()\n",
    "price_per_area_per_region_2.columns = ['zip_code_first3', 'median_price_per_area_2']\n",
    "display(price_per_area_per_region_2.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_per_area_per_region_2.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_per_region_2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, price_per_area_per_region_2, how = 'left', on = 'zip_code_first3')\n",
    "\n",
    "df_train['median_price_per_area_2'] = df_train['median_price_per_area_2'].combine_first(df_train['median_price_per_area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "## Regional prices - replace Statbel\n",
    "\n",
    "Use training data rather than external resource that needs to be updated over time\n",
    "<br><s> Combine with price per area so only one variable is needed </s>\n",
    "<br>Despite the name, price_per_area_type_region in attempt 3 ultimately refers to the median price per type (is_appartment Y/N) per zip code, cf. Statbel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_per_area_type_zipcode = df_train.groupby(['postcode', 'is_appartment'])['price_per_area'].median().round(2).reset_index()\n",
    "\n",
    "price_per_area_type_zipcode = df_train.groupby(['postcode', 'is_appartment'])['price'].median().round(2).reset_index()\n",
    "\n",
    "price_per_area_type_zipcode.columns = ['postcode', 'is_appartment', 'price_per_area_type_zipcode']\n",
    "display(price_per_area_type_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"postcode_first3\"] = df_train.postcode.astype(str).str[:3]\n",
    "\n",
    "# price_per_area_type_zipcodefirst3 = df_train.groupby(['postcode_first3', 'is_appartment'])['price_per_area'].median().round(2).reset_index()\n",
    "\n",
    "price_per_area_type_zipcodefirst3 = df_train.groupby(['postcode_first3', 'is_appartment'])['price'].median().round(2).reset_index()\n",
    "price_per_area_type_zipcodefirst3.columns = ['postcode_first3', 'is_appartment', 'price_per_area_type_zipcodefirst3']\n",
    "display(price_per_area_type_zipcodefirst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"postcode_first2\"] = df_train.postcode.astype(str).str[:2]\n",
    "\n",
    "# price_per_area_type_zipcodefirst2 = df_train.groupby(['postcode_first2', 'is_appartment'])['price_per_area'].median().round(2).reset_index()\n",
    "\n",
    "price_per_area_type_zipcodefirst2 = df_train.groupby(['postcode_first2', 'is_appartment'])['price'].median().round(2).reset_index()\n",
    "price_per_area_type_zipcodefirst2.columns = ['postcode_first2', 'is_appartment', 'price_per_area_type_zipcodefirst2']\n",
    "display(price_per_area_type_zipcodefirst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"postcode_first1\"] = df_train.postcode.astype(str).str[:1]\n",
    "\n",
    "# price_per_area_type_zipcodefirst1 = df_train.groupby(['postcode_first1', 'is_appartment'])['price_per_area'].median().round(2).reset_index()\n",
    "\n",
    "price_per_area_type_zipcodefirst1 = df_train.groupby(['postcode_first1', 'is_appartment'])['price'].median().round(2).reset_index()\n",
    "price_per_area_type_zipcodefirst1.columns = ['postcode_first1', 'is_appartment', 'price_per_area_type_zipcodefirst1']\n",
    "display(price_per_area_type_zipcodefirst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_data/price_per_area_type_zipcode.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_type_zipcode, file)\n",
    "\n",
    "with open('intermediate_data/price_per_area_type_zipcodefirst3.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_type_zipcodefirst3, file)\n",
    "\n",
    "with open('intermediate_data/price_per_area_type_zipcodefirst2.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_type_zipcodefirst2, file)\n",
    "\n",
    "with open('intermediate_data/price_per_area_type_zipcodefirst1.pkl', 'wb') as file:\n",
    "    pickle.dump(price_per_area_type_zipcodefirst1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, price_per_area_type_zipcode, how = 'left', on = ['postcode', 'is_appartment'])\n",
    "df_train = pd.merge(df_train, price_per_area_type_zipcodefirst3, how = 'left', on = ['postcode_first3', 'is_appartment'])\n",
    "df_train = pd.merge(df_train, price_per_area_type_zipcodefirst2, how = 'left', on = ['postcode_first2', 'is_appartment'])\n",
    "df_train = pd.merge(df_train, price_per_area_type_zipcodefirst1, how = 'left', on = ['postcode_first1', 'is_appartment'])\n",
    "df_train['price_per_area_type_region'] = df_train['price_per_area_type_zipcode'].\\\n",
    "combine_first(df_train['price_per_area_type_zipcodefirst3']).\\\n",
    "combine_first(df_train['price_per_area_type_zipcodefirst2']).\\\n",
    "combine_first(df_train['price_per_area_type_zipcodefirst1'])\n",
    "display(df_train[df_train['price_per_area_type_zipcode'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train[df_train['price_per_area_type_region'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "## Lat / Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['lon'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lat_missing'] = df_train['lat'].isna().astype(int)\n",
    "df_train['lon_missing'] = df_train['lon'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "Attempt 3: remove outlying coordinates\n",
    "<br> belgium.be: Belgium spans 2 degrees in latitude, from 51 degrees 30 minutes N at Meerle (northernmost point) to 49 degrees 30 minutes N at Torgny (southernmost point). In longitude, it spans less than 4 degrees, from 2 degrees 33 minutes E to 6 degrees 24 minutes E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['lat'] < 49.5) | (df_train['lat'] > 51.5) | (df_train['lon'] < 2.55) | (df_train['lon'] > 6.4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train['lat'].isna()) | (df_train['lon'].isna())  | ((df_train['lat'] >= 49.5) & (df_train['lat'] <= 51.5) & (df_train['lon'] >= 2.55) & (df_train['lon'] <= 6.4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lat = df_train.groupby('province')['lat'].mean()\n",
    "mean_lat.to_pickle('intermediate_data/mean_lat.pkl')\n",
    "display(mean_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lon = df_train.groupby('province')['lon'].mean()\n",
    "mean_lon.to_pickle('intermediate_data/mean_lon.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lat_imputed'] = df_train.apply(\n",
    "    lambda row: mean_lat.get((row['province']), row['lat']) if pd.isna(row['lat']) else row['lat'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_train['lon_imputed'] = df_train.apply(\n",
    "    lambda row: mean_lon.get((row['province']), row['lon']) if pd.isna(row['lon']) else row['lon'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['lat_imputed'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "## Price drop flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['price_dropped'] = abs(df_train['price_drop_date'].isna().astype(int) - 1)\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "## Zipcode last digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"postcode_last3\"] = df_train.postcode.astype(str).str[1:] \n",
    "df_train[\"postcode_last2\"] = df_train.postcode.astype(str).str[2:]\n",
    "df_train[\"postcode_last3_0\"] = (df_train[\"postcode_last3\"] == '000').astype('int')\n",
    "df_train[\"postcode_last2_0\"] = ((df_train[\"postcode_last3\"] != '000') & (df_train[\"postcode_last2\"] == '00')).astype('int')\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "## Profile again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave out percentiles 25 and 75 for prices per NIS code because of multicollinearity\n",
    "# Leave out row count because it's not available when making a single prediction and has low predictive power (it's usually just 1)\n",
    "\n",
    "df_train_sel = df_train[['bedrooms', 'new_building', 'foto_amount', 'province', 'subtype_regrouped', 'subtype_median_price', 'area_missing', 'area_imputed', 'area_rel_to_bedrooms', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_TOTAL_TRANSACTIONS', 'MS_P_50_median_imputed', 'median_price_per_area', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel = ProfileReport(df_train_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows?\n",
    "\n",
    "df_train_sel.loc[170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_row_indices = [30, 32, 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_sel.index[:10])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "174",
   "metadata": {},
   "source": [
    "# Very few buildings in Vlaams-Brabant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel.iloc[duplicate_row_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel[\n",
    "    (df_train_sel[\"bedrooms\"] == 3.0) &\n",
    "    (df_train_sel[\"new_building\"] == 1) &\n",
    "    (df_train_sel[\"foto_amount\"] == 7.0) &\n",
    "    (df_train_sel[\"province\"] == \"Oost-Vlaanderen\") &\n",
    "    (df_train_sel[\"subtype_regrouped\"] == \"Duplex\") &\n",
    "    (df_train_sel[\"area_imputed\"] == 110.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\n",
    "    (df_train[\"bedrooms\"] == 3.0) &\n",
    "    (df_train[\"new_building\"] == 1) &\n",
    "    (df_train[\"foto_amount\"] == 7.0) &\n",
    "    (df_train[\"province\"] == \"Oost-Vlaanderen\") &\n",
    "    (df_train[\"subtype_regrouped\"] == \"Duplex\") &\n",
    "    (df_train[\"area_imputed\"] == 110.0)\n",
    "].sort_values('added_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['advertiser'] == 'C-Nest Groep') & (df_train['postcode'] == 9870) & (df_train['area'] == 150) & (df_train['subtype'] == 'Woning')].sort_values('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby([\"area\", \"bedrooms\", \"postcode\", \"lat\", \"lon\", \"advertiser\", \"foto_amount\", \"subtype\"])['price'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel[\n",
    "    (df_train_sel[\"bedrooms\"] == 1) &\n",
    "    (df_train_sel[\"new_building\"] == 1) &\n",
    "    (df_train_sel[\"foto_amount\"] == 25) &\n",
    "    (df_train_sel[\"province\"] == \"Oost-Vlaanderen\") &\n",
    "    (df_train_sel[\"subtype_regrouped\"] == \"Appartement\") &\n",
    "    (df_train_sel[\"area_imputed\"] == 57.1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181",
   "metadata": {},
   "source": [
    "## Final variable selection and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_sel = df_train[['new_building', 'foto_amount', 'province', 'subtype_median_price', 'area_missing', 'area_imputed', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_P_50_median_imputed', 'median_price_per_area', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_sel = df_train_sel[df_train_sel['price'] < 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_preprocessed = df_train_sel.drop('price', axis = 1)\n",
    "# X_train_preprocessed = pd.get_dummies(X_train_preprocessed, columns=['subtype_regrouped', 'province'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = df_train_sel.drop('price', axis = 1)\n",
    "X_train_preprocessed = pd.get_dummies(X_train_preprocessed, columns=['province'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preprocessed = df_train_sel['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed.to_csv(\"split_data/train_features_preprocessed.csv\", index=False)\n",
    "y_train_preprocessed.to_csv(\"split_data/train_target_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "## Final variable selection and encoding - lat/lon instead of province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel_2 = df_train[['new_building', 'foto_amount', 'lat_missing', 'lat_imputed', 'lon_missing', 'lon_imputed', 'subtype_median_price', 'area_missing', 'area_imputed', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_P_50_median_imputed', 'median_price_per_area', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel_2 = ProfileReport(df_train_sel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel_2.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed_2 = df_train_sel_2.drop('price', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preprocessed_2 = df_train_sel_2['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed_2.to_csv(\"split_data/train_features_preprocessed_2.csv\", index=False)\n",
    "y_train_preprocessed_2.to_csv(\"split_data/train_target_preprocessed_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "## Final variable selection and encoding - attempt 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel_3 = df_train[['new_building', 'foto_amount', 'lat_missing', 'lat_imputed', 'lon_imputed', 'subtype_median_price', 'area_missing', 'area_imputed', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'price_per_area_type_region', 'median_price_per_area', 'price_dropped', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel_3 = ProfileReport(df_train_sel_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong negative correlation between median price per subtype and median price per area, per type (is appartment Y/N) per region (zipcode)?\n",
    "# No longer the case when switching back to median price (no longer price per area) per type per region\n",
    "\n",
    "profile_sel_3.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "Detected but currently nothing removed \n",
    "<br> First need to determine suitable contamination level (% outliers) and don't just want to remove all high prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest(contamination=0.005)  # expected % of outliers\n",
    "model.fit(df_train_sel_3[['area_imputed', 'energy_value_imputed', 'price']])\n",
    "outliers = model.predict(df_train_sel_3[['area_imputed', 'energy_value_imputed', 'price']])  # returns 1 for inliers, -1 for outliers\n",
    "\n",
    "display(df_train_sel_3[outliers == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed_3 = df_train_sel_3.drop('price', axis = 1)\n",
    "y_train_preprocessed_3 = df_train_sel_3['price']\n",
    "\n",
    "X_train_preprocessed_3.to_csv(\"split_data/train_features_preprocessed_3.csv\", index=False)\n",
    "y_train_preprocessed_3.to_csv(\"split_data/train_target_preprocessed_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {},
   "source": [
    "## Final variable selection and encoding - attempt 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel_3b = df_train[['new_building', 'foto_amount', 'lat_missing', 'lat_imputed', 'lon_imputed', 'subtype_median_price', 'area_missing', 'area_imputed', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_P_50_median_imputed', 'median_price_per_area_2', 'postcode_last2_0', 'postcode_last3_0', 'price_dropped', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel_3b = ProfileReport(df_train_sel_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sel_3b.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed_3b = df_train_sel_3b.drop('price', axis = 1)\n",
    "y_train_preprocessed_3b = df_train_sel_3b['price']\n",
    "\n",
    "X_train_preprocessed_3b.to_csv(\"split_data/train_features_preprocessed_3b.csv\", index=False)\n",
    "y_train_preprocessed_3b.to_csv(\"split_data/train_target_preprocessed_3b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "## Smaller training set for TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sel_3 = df_train[['new_building', 'foto_amount', 'lat_missing', 'lat_imputed', 'lon_missing', 'lon_imputed', 'subtype_median_price', 'area_missing', 'area_imputed', 'energy_value_missing', 'energy_value_imputed', 'advertiser_count', 'median_price_advertiser_bin', 'MS_P_50_median_imputed', 'median_price_per_area', 'price']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train_sel_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = df_train_sel_3.drop('price', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3 = df_train_sel_3['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_1, X_train_3_2, y_train_3_1, y_train_3_2 = train_test_split(X_train_3, y_train_3, test_size=.67, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_2, X_train_3_3, y_train_3_2, y_train_3_3 = train_test_split(X_train_3_2, y_train_3_2, test_size=.5, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_1.to_csv(\"split_data/train_features_preprocessed_3_1.csv\", index=False)\n",
    "y_train_3_1.to_csv(\"split_data/train_target_preprocessed_3_1.csv\", index=False)\n",
    "X_train_3_2.to_csv(\"split_data/train_features_preprocessed_3_2.csv\", index=False)\n",
    "y_train_3_2.to_csv(\"split_data/train_target_preprocessed_3_2.csv\", index=False)\n",
    "X_train_3_3.to_csv(\"split_data/train_features_preprocessed_3_3.csv\", index=False)\n",
    "y_train_3_3.to_csv(\"split_data/train_target_preprocessed_3_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_1, X_train_4_2, y_train_4_1, y_train_4_2 = train_test_split(X_train_3, y_train_3, test_size=.52, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_4_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_2, X_train_4_3, y_train_4_2, y_train_4_3 = train_test_split(X_train_4_2, y_train_4_2, test_size=.08, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_4_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4_1.to_csv(\"split_data/train_features_preprocessed_4_1.csv\", index=False)\n",
    "y_train_4_1.to_csv(\"split_data/train_target_preprocessed_4_1.csv\", index=False)\n",
    "X_train_4_2.to_csv(\"split_data/train_features_preprocessed_4_2.csv\", index=False)\n",
    "y_train_4_2.to_csv(\"split_data/train_target_preprocessed_4_2.csv\", index=False)\n",
    "X_train_4_3.to_csv(\"split_data/train_features_preprocessed_4_3.csv\", index=False)\n",
    "y_train_4_3.to_csv(\"split_data/train_target_preprocessed_4_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
